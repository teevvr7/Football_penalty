{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvTWAz4cg/v9h50Tk//5pI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teevvr7/Football_penalty/blob/main/Small_MODEL(final_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload the zip file from your computer\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename of the uploaded zip (it's a dictionary, this gets the first key)\n",
        "zip_filename = next(iter(uploaded))\n",
        "\n",
        "# Extract the zip file into the Colab environment\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n",
        "\n",
        "# List the contents to confirm it worked\n",
        "data_dir = './penalty_data'\n",
        "print(\"Contents of the data directory:\")\n",
        "print(os.listdir(data_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "uL6thtoH6xJo",
        "outputId": "cfe4de31-996b-4368-c731-116a220e5495"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1f20b1c9-b88f-4ded-92ba-17833e221257\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1f20b1c9-b88f-4ded-92ba-17833e221257\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving penalty_data.zip to penalty_data.zip\n",
            "Contents of the data directory:\n",
            "['NerArg10.mp4', 'NerArg6.mp4', 'NerArg4.mp4', 'FraArg5.mp4', 'BraCro3.mp4', 'FraArg6.mp4', 'JpnCro5.mp4', 'FraArg4.mp4', 'BraCro2.mp4', 'JpnCro6.mp4', 'JpnCro7.mp4', 'FraArg2.mp4', 'NerArg3.mp4', 'NerArg1.mp4', 'BraCro7.mp4', 'NerArg5.mp4', 'JpnCro2.mp4', 'FraArg3.mp4', 'labels.csv', 'BraCro5.mp4', 'MorSpa5.mp4', 'MorSpa4.mp4', 'NerArg8.mp4', 'BraCro8.mp4', 'BraCro1.mp4', 'NerArg2.mp4', 'MorSpa3.mp4', 'JpnCro1.mp4', 'JpnCro3.mp4', 'FraArg1.mp4', 'FraArg8.mp4', 'MorSpa1.mp4', 'BraCro6.mp4', 'MorSpa2.mp4', 'MorSpa6.mp4', 'NerArg7.mp4', 'MorSpa7.mp4', 'JpnCro8.mp4', 'NerArg9.mp4', 'BraCro4.mp4', 'JpnCro4.mp4', 'FraArg7.mp4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === COLAB-COMPATIBLE CODE ===\n",
        "# This version works with Colab's default packages\n",
        "\n",
        "# First, let's check what versions we have and install only what's missing\n",
        "print(\"🔍 Checking current package versions...\")\n",
        "\n",
        "# Import first to see what's available\n",
        "try:\n",
        "    import pandas as pd\n",
        "    print(f\"✅ pandas: {pd.__version__}\")\n",
        "except:\n",
        "    !pip install pandas\n",
        "    import pandas as pd\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "    print(f\"✅ OpenCV: {cv2.__version__}\")\n",
        "except:\n",
        "    !pip install opencv-python-headless\n",
        "    import cv2\n",
        "\n",
        "try:\n",
        "    import sklearn\n",
        "    print(f\"✅ scikit-learn: {sklearn.__version__}\")\n",
        "except:\n",
        "    !pip install scikit-learn\n",
        "    import sklearn\n",
        "\n",
        "try:\n",
        "    import mediapipe as mp\n",
        "    print(f\"✅ mediapipe: {mp.__version__}\")\n",
        "except:\n",
        "    !pip install mediapipe\n",
        "    import mediapipe as mp\n",
        "\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    print(\"✅ ultralytics: Available\")\n",
        "except:\n",
        "    !pip install ultralytics\n",
        "    from ultralytics import YOLO\n",
        "\n",
        "# Now import the rest\n",
        "import os\n",
        "import math\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "\n",
        "print(\"✅ All packages imported successfully!\")\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Load YOLO model for person detection\n",
        "yolo_model = YOLO('yolov8n.pt')\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    \"\"\"Calculates the angle at point b formed by points a, b, c.\"\"\"\n",
        "    a_vec = np.array([a.x - b.x, a.y - b.y])\n",
        "    c_vec = np.array([c.x - b.x, c.y - b.y])\n",
        "    dot_product = np.dot(a_vec, c_vec)\n",
        "    magnitude = np.linalg.norm(a_vec) * np.linalg.norm(c_vec)\n",
        "    if magnitude == 0:\n",
        "        return 0\n",
        "    angle_rad = np.arccos(np.clip(dot_product / magnitude, -1.0, 1.0))\n",
        "    return np.degrees(angle_rad)\n",
        "\n",
        "def find_shooter(frame, yolo_model):\n",
        "    \"\"\"Uses YOLO to find the person most likely to be the shooter\"\"\"\n",
        "    results = yolo_model(frame, verbose=False, conf=0.3)\n",
        "\n",
        "    people_boxes = []\n",
        "    for result in results:\n",
        "        if result.boxes is not None:\n",
        "            for box in result.boxes:\n",
        "                class_id = int(box.cls)\n",
        "                if yolo_model.names[class_id] == 'person':\n",
        "                    people_boxes.append(box.xyxy[0].cpu().numpy())\n",
        "\n",
        "    if not people_boxes:\n",
        "        return None\n",
        "\n",
        "    frame_height, frame_width = frame.shape[:2]\n",
        "    bottom_center = (frame_width // 2, frame_height)\n",
        "\n",
        "    best_box = None\n",
        "    min_distance = float('inf')\n",
        "\n",
        "    for box in people_boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        box_center = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
        "        distance = math.sqrt((box_center[0] - bottom_center[0])**2 +\n",
        "                            (box_center[1] - bottom_center[1])**2)\n",
        "\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            best_box = box\n",
        "\n",
        "    return best_box\n",
        "\n",
        "def smart_crop(frame, best_box, min_size=300):\n",
        "    \"\"\"Smart cropping with resizing for small detections\"\"\"\n",
        "    x1, y1, x2, y2 = best_box.astype(int)\n",
        "\n",
        "    # Add padding\n",
        "    padding = 40\n",
        "    x1 = max(0, x1 - padding)\n",
        "    y1 = max(0, y1 - padding)\n",
        "    x2 = min(frame.shape[1], x2 + padding)\n",
        "    y2 = min(frame.shape[0], y2 + padding)\n",
        "\n",
        "    cropped_frame = frame[y1:y2, x1:x2]\n",
        "\n",
        "    if cropped_frame.size == 0:\n",
        "        return frame\n",
        "\n",
        "    # Resize if too small\n",
        "    height, width = cropped_frame.shape[:2]\n",
        "    if height < min_size or width < min_size:\n",
        "        scale = max(min_size / height, min_size / width)\n",
        "        new_height = int(height * scale)\n",
        "        new_width = int(width * scale)\n",
        "        cropped_frame = cv2.resize(cropped_frame, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    return cropped_frame\n",
        "\n",
        "# Load your CSV file\n",
        "print(\"📊 Loading dataset...\")\n",
        "csv_path = './penalty_data/labels.csv'\n",
        "labels_df = pd.read_csv(csv_path)\n",
        "data_dir = './penalty_data'\n",
        "\n",
        "# Lists to store features and labels\n",
        "feature_list = []\n",
        "label_list = []\n",
        "success_count = 0\n",
        "\n",
        "print(\"Starting feature extraction from videos...\")\n",
        "\n",
        "# Process each video\n",
        "for index, row in labels_df.iterrows():\n",
        "    video_name = row['video_name']\n",
        "    frame_num = row['frame_number']\n",
        "    direction_label = row['shooter_direction']\n",
        "\n",
        "    video_path = os.path.join(data_dir, video_name)\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"⚠️  Video {video_path} not found. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Read the specific frame\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "    success, frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if not success:\n",
        "        print(f\"⚠️  Could not read frame {frame_num} from {video_name}\")\n",
        "        continue\n",
        "\n",
        "    # Find shooter\n",
        "    shooter_box = find_shooter(frame, yolo_model)\n",
        "    if shooter_box is None:\n",
        "        print(f\"⚠️  No person detected in {video_name}\")\n",
        "        continue\n",
        "\n",
        "    # Crop and process\n",
        "    processed_frame = smart_crop(frame, shooter_box)\n",
        "\n",
        "    # Pose detection\n",
        "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.2) as pose:  # Lower confidence threshold\n",
        "        frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(frame_rgb)\n",
        "\n",
        "        if not results.pose_landmarks:\n",
        "            print(f\"⚠️  No pose in {video_name}:{frame_num}\")\n",
        "            continue\n",
        "\n",
        "        landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "        # Extract key features\n",
        "        try:\n",
        "            # Plant foot angles (most important features)\n",
        "            l_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
        "            l_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE]\n",
        "            l_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
        "            plant_left = calculate_angle(l_ankle, l_knee, l_hip)\n",
        "\n",
        "            r_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
        "            r_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
        "            r_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
        "            plant_right = calculate_angle(r_ankle, r_knee, r_hip)\n",
        "\n",
        "            # Body lean\n",
        "            l_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
        "            r_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "            l_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
        "            r_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "            shoulder_center = (l_shoulder.x + r_shoulder.x) / 2\n",
        "            hip_center = (l_hip.x + r_hip.x) / 2\n",
        "            body_lean = shoulder_center - hip_center\n",
        "\n",
        "            # Foot forwardness\n",
        "            foot_forwardness = l_ankle.y - r_ankle.y\n",
        "\n",
        "            # Shoulder-hip angle\n",
        "            shoulder_hip_angle = calculate_angle(l_shoulder, l_hip, l_knee)\n",
        "\n",
        "        except (IndexError, AttributeError) as e:\n",
        "            print(f\"⚠️  Error extracting features: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Create feature vector (top 5 features only)\n",
        "        features = [plant_left, plant_right, foot_forwardness, body_lean, shoulder_hip_angle]\n",
        "        feature_list.append(features)\n",
        "        label_list.append(direction_label)\n",
        "        success_count += 1\n",
        "        print(f\"✅ {video_name}:{frame_num} -> {direction_label}\")\n",
        "\n",
        "print(f\"\\n📈 Successfully processed {success_count} out of {len(labels_df)} samples\")\n",
        "\n",
        "if success_count == 0:\n",
        "    print(\"❌ No samples were processed successfully. Please check your video files.\")\n",
        "else:\n",
        "    # Convert to arrays\n",
        "    X = np.array(feature_list)\n",
        "    y = np.array(label_list)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"🤖 Training K-Nearest Neighbors model...\")\n",
        "    model = KNeighborsClassifier(n_neighbors=3)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = model.score(X_test, y_test)\n",
        "    print(f\"🎯 Model accuracy: {accuracy:.3f}\")\n",
        "\n",
        "    # Save model\n",
        "    model_filename = 'final_penalty_predictor.pkl'\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"💾 Model saved as {model_filename}\")\n",
        "\n",
        "    # Download\n",
        "    files.download(model_filename)\n",
        "    print(\"📦 Model downloaded! Ready for deployment.\")\n",
        "\n",
        "print(\"\\n=== PROCESS COMPLETE ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pqq0seDU4NhQ",
        "outputId": "30245b9e-06e3-42ab-d0f8-1d7b86ccf481"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Checking current package versions...\n",
            "✅ pandas: 2.2.2\n",
            "✅ OpenCV: 4.11.0\n",
            "✅ scikit-learn: 1.6.1\n",
            "✅ mediapipe: 0.10.21\n",
            "✅ ultralytics: Available\n",
            "✅ All packages imported successfully!\n",
            "📊 Loading dataset...\n",
            "Starting feature extraction from videos...\n",
            "⚠️  No pose in BraCro1.mp4:84\n",
            "✅ BraCro2.mp4:55 -> right\n",
            "✅ BraCro3.mp4:79 -> center\n",
            "✅ BraCro4.mp4:142 -> left\n",
            "✅ BraCro5.mp4:75 -> left\n",
            "✅ BraCro6.mp4:79 -> left\n",
            "✅ BraCro7.mp4:133 -> left\n",
            "✅ BraCro8.mp4:143 -> left\n",
            "⚠️  No pose in FraArg1.mp4:101\n",
            "✅ FraArg2.mp4:127 -> left\n",
            "⚠️  No pose in FraArg3.mp4:127\n",
            "✅ FraArg4.mp4:90 -> center\n",
            "✅ FraArg5.mp4:152 -> left\n",
            "✅ FraArg6.mp4:269 -> left\n",
            "✅ FraArg7.mp4:106 -> center\n",
            "✅ FraArg8.mp4:177 -> left\n",
            "✅ JpnCro1.mp4:94 -> right\n",
            "✅ JpnCro2.mp4:32 -> left\n",
            "✅ JpnCro3.mp4:86 -> left\n",
            "✅ JpnCro4.mp4:40 -> center\n",
            "✅ JpnCro5.mp4:89 -> right\n",
            "✅ JpnCro6.mp4:70 -> left\n",
            "✅ JpnCro7.mp4:42 -> left\n",
            "✅ JpnCro8.mp4:72 -> left\n",
            "✅ MorSpa1.mp4:44 -> right\n",
            "✅ MorSpa2.mp4:77 -> right\n",
            "✅ MorSpa3.mp4:117 -> center\n",
            "✅ MorSpa4.mp4:147 -> right\n",
            "✅ MorSpa5.mp4:189 -> left\n",
            "✅ MorSpa6.mp4:139 -> left\n",
            "✅ MorSpa7.mp4:130 -> center\n",
            "✅ NerArg1.mp4:57 -> left\n",
            "✅ NerArg2.mp4:140 -> left\n",
            "✅ NerArg3.mp4:38 -> right\n",
            "✅ NerArg4.mp4:93 -> left\n",
            "✅ NerArg5.mp4:72 -> right\n",
            "✅ NerArg6.mp4:48 -> right\n",
            "✅ NerArg7.mp4:60 -> left\n",
            "✅ NerArg8.mp4:84 -> left\n",
            "✅ NerArg9.mp4:61 -> left\n",
            "✅ NerArg10.mp4:127 -> left\n",
            "\n",
            "📈 Successfully processed 38 out of 41 samples\n",
            "Training set: (30, 5), Test set: (8, 5)\n",
            "🤖 Training K-Nearest Neighbors model...\n",
            "🎯 Model accuracy: 0.625\n",
            "💾 Model saved as final_penalty_predictor.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ef9ca6ce-6b5d-425b-9146-2d82b114921a\", \"final_penalty_predictor.pkl\", 4502)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Model downloaded! Ready for deployment.\n",
            "\n",
            "=== PROCESS COMPLETE ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "RKP8Tm-LsdCz",
        "outputId": "674d3d06-305e-4a96-f51f-f6443e2f7482"
      },
      "source": [
        "# === COMPLETE PENALTY PREDICTION MODEL TRAINING ===\n",
        "# This code will process your videos, extract features, train the model, and save the .pkl file\n",
        "\n",
        "# Install required packages\n",
        "!pip install mediapipe==0.10.9 ultralytics==8.0.20 opencv-python-headless==4.8.0.74 scikit-learn==1.3.2 pandas==1.5.3 numpy==1.23.5 --no-cache-dir --force-reinstall --quiet\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import math\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "from google.colab import files\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Load YOLO model for person detection\n",
        "yolo_model = YOLO('yolov8n.pt')\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    \"\"\"Calculates the angle at point b formed by points a, b, c.\"\"\"\n",
        "    a_vec = np.array([a.x - b.x, a.y - b.y])\n",
        "    c_vec = np.array([c.x - b.x, c.y - b.y])\n",
        "    dot_product = np.dot(a_vec, c_vec)\n",
        "    magnitude = np.linalg.norm(a_vec) * np.linalg.norm(c_vec)\n",
        "    if magnitude == 0:\n",
        "        return 0\n",
        "    angle_rad = np.arccos(np.clip(dot_product / magnitude, -1.0, 1.0))\n",
        "    return np.degrees(angle_rad)\n",
        "\n",
        "def find_shooter(frame, yolo_model):\n",
        "    \"\"\"Uses YOLO to find the person most likely to be the shooter\"\"\"\n",
        "    results = yolo_model(frame, verbose=False, conf=0.3)  # Lower confidence to 0.3\n",
        "\n",
        "    people_boxes = []\n",
        "    for result in results:\n",
        "        if result.boxes is not None:\n",
        "            for box in result.boxes:\n",
        "                class_id = int(box.cls)\n",
        "                if yolo_model.names[class_id] == 'person':\n",
        "                    people_boxes.append(box.xyxy[0].cpu().numpy())\n",
        "\n",
        "    if not people_boxes:\n",
        "        return None\n",
        "\n",
        "    frame_height, frame_width = frame.shape[:2]\n",
        "    bottom_center = (frame_width // 2, frame_height)\n",
        "\n",
        "    best_box = None\n",
        "    min_distance = float('inf')\n",
        "\n",
        "    for box in people_boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        box_center = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
        "        distance = math.sqrt((box_center[0] - bottom_center[0])**2 +\n",
        "                            (box_center[1] - bottom_center[1])**2)\n",
        "\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            best_box = box\n",
        "\n",
        "    return best_box\n",
        "\n",
        "def smart_crop(frame, best_box, min_size=400):\n",
        "    \"\"\"\n",
        "    Crops the frame around the detected person, but ensures the result\n",
        "    is large enough for pose detection by resizing if necessary.\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = best_box.astype(int)\n",
        "\n",
        "    # Add generous padding (proportional to person size)\n",
        "    person_height = y2 - y1\n",
        "    person_width = x2 - x1\n",
        "    padding_y = int(person_height * 1.2)  # 120% padding\n",
        "    padding_x = int(person_width * 1.2)   # 120% padding\n",
        "\n",
        "    x1 = max(0, x1 - padding_x)\n",
        "    y1 = max(0, y1 - padding_y)\n",
        "    x2 = min(frame.shape[1], x2 + padding_x)\n",
        "    y2 = min(frame.shape[0], y2 + padding_y)\n",
        "\n",
        "    cropped_frame = frame[y1:y2, x1:x2]\n",
        "\n",
        "    # If cropping resulted in empty frame, return original frame\n",
        "    if cropped_frame.size == 0:\n",
        "        return frame\n",
        "\n",
        "    # Resize if too small\n",
        "    height, width = cropped_frame.shape[:2]\n",
        "    if height < min_size or width < min_size:\n",
        "        scale = max(min_size / height, min_size / width)\n",
        "        new_height = int(height * scale)\n",
        "        new_width = int(width * scale)\n",
        "        cropped_frame = cv2.resize(cropped_frame, (new_width, new_height),\n",
        "                                  interpolation=cv2.INTER_CUBIC)\n",
        "        print(f\"  Resized from {height}x{width} to {new_height}x{new_width}\")\n",
        "\n",
        "    return cropped_frame\n",
        "\n",
        "# Load your CSV file\n",
        "csv_path = './penalty_data/labels.csv'\n",
        "labels_df = pd.read_csv(csv_path)\n",
        "data_dir = './penalty_data'\n",
        "\n",
        "# Lists to store our features and labels\n",
        "feature_list = []\n",
        "label_list = []\n",
        "success_count = 0\n",
        "fail_count = 0\n",
        "\n",
        "print(\"Starting feature extraction from videos...\")\n",
        "\n",
        "# Loop through each row in the CSV\n",
        "for index, row in labels_df.iterrows():\n",
        "    video_name = row['video_name']\n",
        "    frame_num = row['frame_number']\n",
        "    direction_label = row['shooter_direction']\n",
        "\n",
        "    video_path = os.path.join(data_dir, video_name)\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Warning: Video file {video_path} not found. Skipping.\")\n",
        "        fail_count += 1\n",
        "        continue\n",
        "\n",
        "    # Open the video and seek to the specific frame\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "    success, original_frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if not success:\n",
        "        print(f\"Could not read frame {frame_num} from {video_name}. Skipping.\")\n",
        "        fail_count += 1\n",
        "        continue\n",
        "\n",
        "    # Use YOLO to find the shooter\n",
        "    shooter_box = find_shooter(original_frame, yolo_model)\n",
        "\n",
        "    if shooter_box is None:\n",
        "        print(f\"No person detected in {video_name} at frame {frame_num}. Skipping.\")\n",
        "        fail_count += 1\n",
        "        continue\n",
        "\n",
        "    # Use SMART cropping\n",
        "    processed_frame = smart_crop(original_frame, shooter_box, min_size=400)\n",
        "\n",
        "    # Try pose detection on the processed frame\n",
        "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3) as pose:\n",
        "        frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(frame_rgb)\n",
        "\n",
        "        if not results.pose_landmarks:\n",
        "            print(f\"No pose detected in processed frame of {video_name}:{frame_num}. Skipping.\")\n",
        "            fail_count += 1\n",
        "            continue\n",
        "\n",
        "        landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "        # --- FEATURE EXTRACTION ---\n",
        "        try:\n",
        "            # 1. Plant Foot Angles\n",
        "            l_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
        "            l_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE]\n",
        "            l_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
        "            plant_foot_angle_left = calculate_angle(l_ankle, l_knee, l_hip)\n",
        "\n",
        "            r_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
        "            r_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
        "            r_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
        "            plant_foot_angle_right = calculate_angle(r_ankle, r_knee, r_hip)\n",
        "        except (IndexError, AttributeError):\n",
        "            plant_foot_angle_left = 0\n",
        "            plant_foot_angle_right = 0\n",
        "\n",
        "        # 2. Hip Orientation\n",
        "        try:\n",
        "            l_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
        "            l_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
        "            l_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE]\n",
        "            shoulder_hip_angle = calculate_angle(l_shoulder, l_hip, l_knee)\n",
        "        except (IndexError, AttributeError):\n",
        "            shoulder_hip_angle = 0\n",
        "\n",
        "        # 3. Torso Lean\n",
        "        try:\n",
        "            nose_y = landmarks[mp_pose.PoseLandmark.NOSE].y\n",
        "            l_hip_y = landmarks[mp_pose.PoseLandmark.LEFT_HIP].y\n",
        "            torso_lean = nose_y - l_hip_y\n",
        "        except (IndexError, AttributeError):\n",
        "            torso_lean = 0\n",
        "\n",
        "        # 4. Hip-Shoulder Separation\n",
        "        try:\n",
        "            l_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
        "            r_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "            l_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
        "            r_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "            shoulder_angle = math.degrees(math.atan2(r_shoulder.y - l_shoulder.y, r_shoulder.x - l_shoulder.x))\n",
        "            hip_angle = math.degrees(math.atan2(r_hip.y - l_hip.y, r_hip.x - l_hip.x))\n",
        "            hip_shoulder_separation = abs(shoulder_angle - hip_angle)\n",
        "        except (IndexError, AttributeError):\n",
        "            hip_shoulder_separation = 0\n",
        "\n",
        "        # 5. Stance Width\n",
        "        try:\n",
        "            l_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
        "            r_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
        "            stance_width = abs(l_ankle.x - r_ankle.x)\n",
        "        except (IndexError, AttributeError):\n",
        "            stance_width = 0\n",
        "\n",
        "        # 6. Foot Forwardness\n",
        "        try:\n",
        "            l_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
        "            r_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
        "            foot_forwardness = l_ankle.y - r_ankle.y\n",
        "        except (IndexError, AttributeError):\n",
        "            foot_forwardness = 0\n",
        "\n",
        "        # 7. Body Lean\n",
        "        try:\n",
        "            l_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
        "            r_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
        "            l_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
        "            r_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "            shoulder_center_x = (l_shoulder.x + r_shoulder.x) / 2\n",
        "            hip_center_x = (l_hip.x + r_hip.x) / 2\n",
        "            body_lean = shoulder_center_x - hip_center_x\n",
        "        except (IndexError, AttributeError):\n",
        "            body_lean = 0\n",
        "\n",
        "        # Create feature vector\n",
        "        features = [\n",
        "            plant_foot_angle_left,\n",
        "            plant_foot_angle_right,\n",
        "            shoulder_hip_angle,\n",
        "            torso_lean,\n",
        "            hip_shoulder_separation,\n",
        "            stance_width,\n",
        "            foot_forwardness,\n",
        "            body_lean\n",
        "        ]\n",
        "\n",
        "        # Append the ORIGINAL features and label\n",
        "        feature_list.append(features)\n",
        "        label_list.append(direction_label)\n",
        "        success_count += 1\n",
        "        print(f\"✅ Processed {video_name}:{frame_num} -> {direction_label}\")\n",
        "\n",
        "        # --- DATA AUGMENTATION: Create MIRRORED version ---\n",
        "        if direction_label == \"left\":\n",
        "            mirrored_label = \"right\"\n",
        "        elif direction_label == \"right\":\n",
        "            mirrored_label = \"left\"\n",
        "        else:\n",
        "            mirrored_label = \"center\"\n",
        "\n",
        "        # For mirroring, we swap left/right specific features and invert some\n",
        "        mirrored_features = [\n",
        "            plant_foot_angle_right,  # swapped\n",
        "            plant_foot_angle_left,   # swapped\n",
        "            shoulder_hip_angle,\n",
        "            torso_lean,\n",
        "            hip_shoulder_separation,\n",
        "            stance_width,\n",
        "            -foot_forwardness,       # inverted\n",
        "            -body_lean               # inverted\n",
        "        ]\n",
        "\n",
        "        feature_list.append(mirrored_features)\n",
        "        label_list.append(mirrored_label)\n",
        "        print(f\"✅ Created mirrored version -> {mirrored_label}\")\n",
        "\n",
        "print(f\"\\nExtraction complete! Results:\")\n",
        "print(f\"✅ Successful extractions: {success_count}\")\n",
        "print(f\"❌ Failed extractions: {fail_count}\")\n",
        "print(f\"📊 Total samples: {len(feature_list)}\")\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X = np.array(feature_list)\n",
        "y = np.array(label_list)\n",
        "\n",
        "print(f\"\\nFinal dataset shape: {X.shape}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# --- TRAIN FINAL MODEL ON TOP 5 FEATURES ---\n",
        "# Use only the most important features\n",
        "important_indices = [0, 1, 6, 7, 2]  # Plant_Foot_Left, Plant_Foot_Right, Foot_Forwardness, Body_Lean, Shoulder_Hip_Angle\n",
        "X_reduced = X[:, important_indices]\n",
        "\n",
        "# Split the reduced data\n",
        "X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(\n",
        "    X_reduced, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Reduced dataset shape: {X_reduced.shape}\")\n",
        "\n",
        "# Train final KNN model (your best performer)\n",
        "final_model = KNeighborsClassifier(n_neighbors=3)\n",
        "final_model.fit(X_train_reduced, y_train_reduced)\n",
        "\n",
        "# Evaluate\n",
        "final_accuracy = final_model.score(X_test_reduced, y_test_reduced)\n",
        "print(f\"🎯 FINAL MODEL ACCURACY: {final_accuracy:.3f}\")\n",
        "\n",
        "# --- SAVE THE MODEL ---\n",
        "final_model_filename = 'final_penalty_predictor.pkl'\n",
        "joblib.dump(final_model, final_model_filename)\n",
        "print(f\"✅ Model saved as {final_model_filename}\")\n",
        "\n",
        "# Download the model to your computer\n",
        "files.download(final_model_filename)\n",
        "print(\"📦 Model downloaded! You can now use it for deployment.\")\n",
        "\n",
        "# --- CREATE LEFT/RIGHT ONLY CLASSIFIER ---\n",
        "left_right_mask = (y == 'left') | (y == 'right')\n",
        "X_lr = X_reduced[left_right_mask]\n",
        "y_lr = y[left_right_mask]\n",
        "\n",
        "lr_classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "lr_classifier.fit(X_lr, y_lr)\n",
        "lr_accuracy = lr_classifier.score(X_lr, y_lr)\n",
        "print(f\"Left/Right only accuracy: {lr_accuracy:.3f}\")\n",
        "\n",
        "# Save this classifier too\n",
        "lr_model_filename = 'left_right_predictor.pkl'\n",
        "joblib.dump(lr_classifier, lr_model_filename)\n",
        "files.download(lr_model_filename)\n",
        "\n",
        "print(\"\\n=== TRAINING COMPLETE ===\")\n",
        "print(\"You now have two models:\")\n",
        "print(\"1. final_penalty_predictor.pkl - For left/right/center prediction\")\n",
        "print(\"2. left_right_predictor.pkl - For high-confidence left/right prediction\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement mediapipe==0.10.9 (from versions: 0.10.13, 0.10.14, 0.10.15, 0.10.18, 0.10.20, 0.10.21)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for mediapipe==0.10.9\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 86.7MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL ultralytics.nn.tasks.DetectionModel was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ultralytics.nn.tasks.DetectionModel])` or the `torch.serialization.safe_globals([ultralytics.nn.tasks.DetectionModel])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2077569414.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Load YOLO model for person detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0myolo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov8n.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_angle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_ckpt_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;34m\"\"\"Loads a single model weights.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m     \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_safe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDEFAULT_CFG_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# combine model and default args, preferring model args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ema'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0;34m'ultralytics.yolo.v8'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ultralytics.models.yolo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 'ultralytics.yolo.data': 'ultralytics.data'}):  # for legacy 8.0 Classify and Pose models\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m  \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# e.name is missing module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1527\u001b[0m                         )\n\u001b[1;32m   1528\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 return _load(\n\u001b[1;32m   1531\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL ultralytics.nn.tasks.DetectionModel was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ultralytics.nn.tasks.DetectionModel])` or the `torch.serialization.safe_globals([ultralytics.nn.tasks.DetectionModel])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    }
  ]
}